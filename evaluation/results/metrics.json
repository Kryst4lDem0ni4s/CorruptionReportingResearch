{
  "_schema_version": "1.0.0",
  "_description": "Evaluation metrics computed from benchmark runs. This file serves as a template showing the expected structure. Actual results are generated by run_evaluation.py",
  "_timestamp": "YYYY-MM-DDTHH:MM:SSZ",
  "_generated_by": "evaluation.run_evaluation",
  
  "experiment_metadata": {
    "experiment_id": "string - unique identifier",
    "timestamp": "string - ISO 8601 timestamp",
    "duration_seconds": "float - total evaluation duration",
    "system_info": {
      "python_version": "string",
      "torch_version": "string",
      "platform": "string",
      "cpu_count": "int",
      "memory_total_mb": "float"
    },
    "configuration": {
      "num_test_samples": "int",
      "models_evaluated": ["array of model names"],
      "test_datasets": ["array of dataset names"]
    }
  },
  
  "deepfake_detection": {
    "description": "Metrics for image/video deepfake detection performance",
    "dataset": "string - dataset name (e.g., FaceForensics++, Celeb-DF)",
    "num_samples": {
      "real": "int - number of real samples",
      "fake": "int - number of fake samples",
      "total": "int - total samples"
    },
    "metrics": {
      "accuracy": "float [0-1] - overall accuracy",
      "precision": "float [0-1] - true positives / (true positives + false positives)",
      "recall": "float [0-1] - true positives / (true positives + false negatives)",
      "f1_score": "float [0-1] - harmonic mean of precision and recall",
      "auroc": "float [0-1] - area under ROC curve (target: >=0.75, ideal: >=0.90)",
      "auprc": "float [0-1] - area under precision-recall curve",
      "false_positive_rate": "float [0-1]",
      "false_negative_rate": "float [0-1]",
      "specificity": "float [0-1] - true negatives / (true negatives + false positives)",
      "balanced_accuracy": "float [0-1] - average of recall and specificity"
    },
    "confusion_matrix": {
      "true_positive": "int",
      "true_negative": "int",
      "false_positive": "int",
      "false_negative": "int"
    },
    "per_model_performance": {
      "clip": {
        "accuracy": "float",
        "auroc": "float",
        "inference_time_ms": "float"
      }
    },
    "confidence_intervals": {
      "auroc_ci_95": ["float - lower bound", "float - upper bound"],
      "accuracy_ci_95": ["float - lower bound", "float - upper bound"]
    }
  },
  
  "coordination_detection": {
    "description": "Metrics for detecting coordinated attack campaigns",
    "num_campaigns": "int - number of synthetic attack campaigns tested",
    "num_submissions": "int - total submissions analyzed",
    "metrics": {
      "precision": "float [0-1] - correctly identified coordinated groups / all identified groups",
      "recall": "float [0-1] - correctly identified coordinated groups / all actual groups",
      "f1_score": "float [0-1]",
      "accuracy": "float [0-1]",
      "false_alarm_rate": "float [0-1] - false positives among legitimate submissions"
    },
    "graph_analysis": {
      "avg_clustering_coefficient": "float [0-1]",
      "avg_community_size": "float",
      "num_communities_detected": "int",
      "modularity_score": "float [-0.5 to 1]"
    },
    "feature_importance": {
      "stylometric_similarity": "float [0-1] - contribution weight",
      "temporal_proximity": "float [0-1]",
      "content_overlap": "float [0-1]"
    }
  },
  
  "consensus_simulation": {
    "description": "Byzantine consensus validator simulation metrics",
    "num_validators": "int - total validators in pool",
    "num_submissions_tested": "int",
    "metrics": {
      "convergence_rate": "float [0-1] - proportion reaching consensus",
      "avg_convergence_time_ms": "float - average time to reach consensus",
      "agreement_score": "float [0-1] - validator agreement level",
      "fault_tolerance": "float [0-1] - system robustness to malicious validators"
    },
    "voting_analysis": {
      "unanimous_votes": "int",
      "majority_votes": "int",
      "split_votes": "int",
      "avg_confidence": "float [0-1]"
    },
    "devils_advocate_impact": {
      "challenges_raised": "int",
      "successful_challenges": "int",
      "false_positive_prevention": "float [0-1]"
    }
  },
  
  "counter_evidence_system": {
    "description": "Counter-evidence processing and Bayesian aggregation metrics",
    "num_accusation_defense_pairs": "int",
    "metrics": {
      "false_positive_reduction": "float [0-1] - reduction after counter-evidence (target: >=0.20)",
      "credibility_score_change_avg": "float - average change in credibility after defense",
      "identity_verification_bonus_applied": "int - count of verified defenses",
      "bayesian_aggregation_accuracy": "float [0-1]"
    },
    "presumption_of_innocence": {
      "weighting_factor": 1.3,
      "cases_benefited": "int",
      "avg_score_adjustment": "float"
    },
    "outcome_distribution": {
      "accusations_upheld": "int",
      "accusations_overturned": "int",
      "accusations_weakened": "int",
      "inconclusive": "int"
    }
  },
  
  "system_performance": {
    "description": "Overall system performance benchmarks",
    "latency": {
      "end_to_end_mean_ms": "float",
      "end_to_end_p95_ms": "float",
      "end_to_end_p99_ms": "float",
      "layer_breakdown": {
        "layer1_anonymity_ms": "float",
        "layer2_credibility_ms": "float",
        "layer3_coordination_ms": "float",
        "layer4_consensus_ms": "float",
        "layer5_counter_evidence_ms": "float",
        "layer6_reporting_ms": "float"
      }
    },
    "throughput": {
      "requests_per_second": "float",
      "concurrent_capacity": "int - max concurrent submissions",
      "success_rate": "float [0-1]"
    },
    "memory": {
      "baseline_mb": "float",
      "peak_mb": "float",
      "average_mb": "float",
      "model_footprint_mb": {
        "clip": "float",
        "sentence_transformer": "float",
        "total": "float"
      },
      "memory_leak_detected": "boolean"
    },
    "storage": {
      "total_submissions": "int",
      "total_evidence_files": "int",
      "disk_usage_mb": "float",
      "avg_submission_size_kb": "float"
    }
  },
  
  "comparative_analysis": {
    "description": "Comparison against paper targets and baselines",
    "deepfake_detection_vs_target": {
      "achieved_auroc": "float",
      "target_auroc": 0.90,
      "gap": "float - (achieved - target)",
      "meets_minimum": "boolean - achieved >= 0.75"
    },
    "counter_evidence_vs_target": {
      "achieved_fp_reduction": "float",
      "target_fp_reduction": 0.20,
      "gap": "float",
      "meets_target": "boolean"
    },
    "cost_analysis": {
      "infrastructure_cost_usd": 0.0,
      "model_training_cost_usd": 0.0,
      "compute_cost_per_submission_usd": 0.0,
      "deployment_complexity": "string - low/medium/high"
    }
  },
  
  "research_findings": {
    "description": "Key findings for publication",
    "strengths": [
      "Array of strings describing system strengths"
    ],
    "limitations": [
      "Array of strings describing limitations"
    ],
    "novel_contributions": [
      "Array of strings describing novel contributions"
    ],
    "future_work": [
      "Array of strings describing future research directions"
    ]
  },
  
  "visualization_files": {
    "description": "Generated visualization files",
    "roc_curve": "figures/roc_curve_TIMESTAMP.png",
    "confusion_matrix": "figures/confusion_matrix_TIMESTAMP.png",
    "coordination_network": "figures/coordination_network_TIMESTAMP.png",
    "score_distribution": "figures/score_distribution_TIMESTAMP.png",
    "latency_breakdown": "figures/latency_breakdown_TIMESTAMP.png",
    "memory_usage": "figures/memory_usage_TIMESTAMP.png"
  },
  
  "_notes": [
    "All float values in range [0-1] unless otherwise specified",
    "Timestamps in ISO 8601 format (YYYY-MM-DDTHH:MM:SSZ)",
    "This is a schema template - actual values generated by evaluation scripts",
    "Generated by: python -m evaluation.run_evaluation",
    "Confidence intervals calculated using bootstrap resampling (n=1000)"
  ]
}
