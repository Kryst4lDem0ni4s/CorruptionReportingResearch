# Corruption Reporting System - Evaluation Analysis Report

**Experiment ID:** `{experiment_id}`  
**Date Generated:** `{timestamp}`  
**Evaluation Duration:** `{duration_seconds}` seconds  
**System Version:** `{version}`

---

## Executive Summary

> **Note:** This is a template for the analysis report. Actual findings are generated by `python -m evaluation.run_evaluation` and will be populated based on real experimental results.

### Key Findings

- **Deepfake Detection Performance:** AUROC of `{auroc:.3f}` (Target: ≥0.75, Ideal: ≥0.90)
- **Coordination Detection:** F1-Score of `{coordination_f1:.3f}` 
- **Counter-Evidence Impact:** `{fp_reduction:.1%}` false positive reduction (Target: ≥20%)
- **System Throughput:** `{throughput:.2f}` requests/second
- **Zero-Cost Achievement:** $0 infrastructure and training costs ✓

---

## 1. Experimental Setup

### 1.1 Dataset Configuration

| Dataset | Purpose | Samples (Real/Fake) | Source |
|---------|---------|---------------------|--------|
| FaceForensics++ | Deepfake Detection | `{ff_real}/{ff_fake}` | Official Dataset |
| Celeb-DF v2 | Deepfake Detection | `{cd_real}/{cd_fake}` | Official Dataset |
| Synthetic Attacks | Coordination Detection | `{synth_total}` campaigns | Generated |

### 1.2 System Configuration

- **Python Version:** `{python_version}`
- **PyTorch Version:** `{torch_version}`
- **Hardware:** `{cpu_count}` CPU cores, `{memory_total_mb:.0f}` MB RAM
- **Models Used:** CLIP, Sentence Transformer
- **Deployment:** Single-machine, file-based storage

### 1.3 Evaluation Metrics

#### Deepfake Detection
- AUROC (Area Under ROC Curve)
- Precision, Recall, F1-Score
- Confusion Matrix Analysis
- Confidence Intervals (95%)

#### Coordination Detection
- Precision/Recall for attack identification
- Graph-based metrics (modularity, clustering)
- False alarm rate on legitimate submissions

#### Consensus Simulation
- Convergence rate and time
- Validator agreement scores
- Fault tolerance analysis

#### Counter-Evidence System
- False positive reduction rate
- Credibility score adjustments
- Bayesian aggregation accuracy

---

## 2. Results

### 2.1 Deepfake Detection Performance

#### Overall Metrics

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| AUROC | `{auroc:.3f}` | ≥0.75 | `{auroc_status}` |
| Accuracy | `{accuracy:.3f}` | - | - |
| Precision | `{precision:.3f}` | - | - |
| Recall | `{recall:.3f}` | - | - |
| F1-Score | `{f1:.3f}` | - | - |

**Confidence Intervals (95%):**
- AUROC: [`{auroc_ci_lower:.3f}`, `{auroc_ci_upper:.3f}`]
- Accuracy: [`{acc_ci_lower:.3f}`, `{acc_ci_upper:.3f}`]

#### Confusion Matrix

