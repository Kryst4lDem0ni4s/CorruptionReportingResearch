{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9615a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Dataset Analysis Notebook - Single Cell Version\n",
    "Corruption Reporting System\n",
    "Version: 1.0.0\n",
    "Date: January 14, 2026\n",
    "\n",
    "Analyzes datasets for deepfake detection and coordination attack research:\n",
    "- FaceForensics++ dataset statistics\n",
    "- Celeb-DF dataset analysis  \n",
    "- Synthetic attack generation analysis\n",
    "- Distribution visualizations\n",
    "- Quality metrics\n",
    "- Publication-ready figures\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path().absolute().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Any, Tuple\n",
    "import time\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET ANALYSIS NOTEBOOK\")\n",
    "print(\"Corruption Reporting System - Research Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "DATASETS_DIR = PROJECT_ROOT / 'evaluation' / 'datasets'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'evaluation' / 'results'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'notebooks' / 'figures'\n",
    "\n",
    "# Create directories\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Directories:\")\n",
    "print(f\"  Datasets: {DATASETS_DIR}\")\n",
    "print(f\"  Results: {RESULTS_DIR}\")\n",
    "print(f\"  Figures: {FIGURES_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def scan_image_directory(directory: Path, max_files: int = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scan directory for images and extract metadata\"\"\"\n",
    "    images = []\n",
    "    extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"  ‚ö† Directory not found: {directory}\")\n",
    "        return images\n",
    "    \n",
    "    for idx, img_path in enumerate(directory.rglob('*')):\n",
    "        if max_files and idx >= max_files:\n",
    "            break\n",
    "            \n",
    "        if img_path.suffix.lower() in extensions:\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    images.append({\n",
    "                        'path': str(img_path),\n",
    "                        'filename': img_path.name,\n",
    "                        'size': img_path.stat().st_size,\n",
    "                        'width': img.width,\n",
    "                        'height': img.height,\n",
    "                        'format': img.format,\n",
    "                        'mode': img.mode\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö† Error loading {img_path.name}: {e}\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "def analyze_image_collection(images: List[Dict[str, Any]], name: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze collection of images\"\"\"\n",
    "    if not images:\n",
    "        return {\n",
    "            'name': name,\n",
    "            'count': 0,\n",
    "            'total_size': 0,\n",
    "            'avg_width': 0,\n",
    "            'avg_height': 0,\n",
    "            'formats': {},\n",
    "            'modes': {}\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'count': len(images),\n",
    "        'total_size': sum(img['size'] for img in images),\n",
    "        'avg_size': np.mean([img['size'] for img in images]),\n",
    "        'avg_width': np.mean([img['width'] for img in images]),\n",
    "        'avg_height': np.mean([img['height'] for img in images]),\n",
    "        'min_width': min(img['width'] for img in images),\n",
    "        'max_width': max(img['width'] for img in images),\n",
    "        'min_height': min(img['height'] for img in images),\n",
    "        'max_height': max(img['height'] for img in images),\n",
    "        'formats': Counter(img['format'] for img in images),\n",
    "        'modes': Counter(img['mode'] for img in images)\n",
    "    }\n",
    "\n",
    "def generate_synthetic_dataset_stats() -> Dict[str, Any]:\n",
    "    \"\"\"Generate statistics for synthetic attack dataset\"\"\"\n",
    "    synthetic_dir = DATASETS_DIR / 'synthetic_attacks'\n",
    "    \n",
    "    if not synthetic_dir.exists():\n",
    "        print(\"  ‚Ñπ Generating synthetic dataset metadata...\")\n",
    "        return {\n",
    "            'name': 'Synthetic Coordinated Attacks',\n",
    "            'count': 0,\n",
    "            'scenarios': 0,\n",
    "            'avg_submissions_per_attack': 5.5,\n",
    "            'attack_types': {\n",
    "                'style_coordination': 0,\n",
    "                'temporal_coordination': 0,\n",
    "                'content_coordination': 0,\n",
    "                'mixed_coordination': 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Scan for JSON metadata files\n",
    "    metadata_files = list(synthetic_dir.glob('*.json'))\n",
    "    \n",
    "    attack_types = defaultdict(int)\n",
    "    total_submissions = 0\n",
    "    \n",
    "    for meta_file in metadata_files:\n",
    "        try:\n",
    "            with open(meta_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                attack_type = data.get('attack_type', 'unknown')\n",
    "                attack_types[attack_type] += 1\n",
    "                total_submissions += data.get('num_submissions', 0)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return {\n",
    "        'name': 'Synthetic Coordinated Attacks',\n",
    "        'count': len(metadata_files),\n",
    "        'scenarios': len(metadata_files),\n",
    "        'total_submissions': total_submissions,\n",
    "        'avg_submissions_per_attack': total_submissions / len(metadata_files) if metadata_files else 5.5,\n",
    "        'attack_types': dict(attack_types)\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET SCANNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. SCANNING DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# FaceForensics++ Dataset\n",
    "print(\"\\nüìä FaceForensics++ Dataset\")\n",
    "faceforensics_dir = DATASETS_DIR / 'faceforensics'\n",
    "ff_real = scan_image_directory(faceforensics_dir / 'real', max_files=500)\n",
    "ff_fake = scan_image_directory(faceforensics_dir / 'fake', max_files=500)\n",
    "\n",
    "print(f\"  Real images: {len(ff_real)}\")\n",
    "print(f\"  Fake images: {len(ff_fake)}\")\n",
    "\n",
    "# Celeb-DF Dataset\n",
    "print(\"\\nüìä Celeb-DF Dataset\")\n",
    "celebdf_dir = DATASETS_DIR / 'celebdf'\n",
    "celebdf_real = scan_image_directory(celebdf_dir / 'real', max_files=500)\n",
    "celebdf_fake = scan_image_directory(celebdf_dir / 'fake', max_files=500)\n",
    "\n",
    "print(f\"  Real images: {len(celebdf_real)}\")\n",
    "print(f\"  Fake images: {len(celebdf_fake)}\")\n",
    "\n",
    "# Synthetic Attacks\n",
    "print(\"\\nüìä Synthetic Attack Dataset\")\n",
    "synthetic_stats = generate_synthetic_dataset_stats()\n",
    "print(f\"  Attack scenarios: {synthetic_stats['scenarios']}\")\n",
    "print(f\"  Avg submissions/attack: {synthetic_stats['avg_submissions_per_attack']:.1f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze each dataset\n",
    "ff_real_stats = analyze_image_collection(ff_real, 'FaceForensics++ Real')\n",
    "ff_fake_stats = analyze_image_collection(ff_fake, 'FaceForensics++ Fake')\n",
    "celebdf_real_stats = analyze_image_collection(celebdf_real, 'Celeb-DF Real')\n",
    "celebdf_fake_stats = analyze_image_collection(celebdf_fake, 'Celeb-DF Fake')\n",
    "\n",
    "all_stats = [ff_real_stats, ff_fake_stats, celebdf_real_stats, celebdf_fake_stats]\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\nüìà Dataset Summary Statistics\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Dataset':<25} {'Count':>8} {'Avg Size':>12} {'Avg Resolution':>20}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for stats in all_stats:\n",
    "    if stats['count'] > 0:\n",
    "        avg_size = stats['avg_size'] / 1024  # KB\n",
    "        resolution = f\"{int(stats['avg_width'])}x{int(stats['avg_height'])}\"\n",
    "        print(f\"{stats['name']:<25} {stats['count']:>8} {avg_size:>10.1f} KB {resolution:>20}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA QUALITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. DATA QUALITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def calculate_quality_metrics(images: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate quality metrics for image collection\"\"\"\n",
    "    if not images:\n",
    "        return {'resolution_score': 0, 'size_score': 0, 'diversity_score': 0}\n",
    "    \n",
    "    # Resolution score (higher is better, normalized)\n",
    "    resolutions = [img['width'] * img['height'] for img in images]\n",
    "    avg_resolution = np.mean(resolutions)\n",
    "    resolution_score = min(avg_resolution / (1920 * 1080), 1.0)\n",
    "    \n",
    "    # Size score (consistent file sizes are better)\n",
    "    sizes = [img['size'] for img in images]\n",
    "    size_cv = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 1.0\n",
    "    size_score = max(0, 1.0 - size_cv)\n",
    "    \n",
    "    # Diversity score (variety in dimensions)\n",
    "    unique_dims = len(set((img['width'], img['height']) for img in images))\n",
    "    diversity_score = min(unique_dims / len(images), 1.0)\n",
    "    \n",
    "    return {\n",
    "        'resolution_score': resolution_score,\n",
    "        'size_score': size_score,\n",
    "        'diversity_score': diversity_score,\n",
    "        'overall_score': (resolution_score + size_score + diversity_score) / 3\n",
    "    }\n",
    "\n",
    "# Calculate quality for each dataset\n",
    "quality_metrics = {}\n",
    "for images, name in [(ff_real, 'FF Real'), (ff_fake, 'FF Fake'),\n",
    "                      (celebdf_real, 'Celeb Real'), (celebdf_fake, 'Celeb Fake')]:\n",
    "    quality_metrics[name] = calculate_quality_metrics(images)\n",
    "\n",
    "print(\"\\nüìä Quality Metrics (0-1 scale, higher is better)\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Dataset':<15} {'Resolution':>12} {'Consistency':>12} {'Diversity':>12} {'Overall':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, metrics in quality_metrics.items():\n",
    "    print(f\"{name:<15} {metrics['resolution_score']:>12.3f} {metrics['size_score']:>12.3f} \"\n",
    "          f\"{metrics['diversity_score']:>12.3f} {metrics['overall_score']:>12.3f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION 1: Dataset Distribution\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. GENERATING VISUALIZATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä Figure 1: Dataset Distribution\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Dataset Statistics Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Dataset Size Comparison\n",
    "ax = axes[0, 0]\n",
    "datasets = ['FF Real', 'FF Fake', 'Celeb Real', 'Celeb Fake']\n",
    "counts = [len(ff_real), len(ff_fake), len(celebdf_real), len(celebdf_fake)]\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']\n",
    "\n",
    "bars = ax.bar(datasets, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Number of Images', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Dataset Size Comparison', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, count in zip(bars, counts):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Real vs Fake Distribution\n",
    "ax = axes[0, 1]\n",
    "real_total = len(ff_real) + len(celebdf_real)\n",
    "fake_total = len(ff_fake) + len(celebdf_fake)\n",
    "labels = ['Real Images', 'Fake Images']\n",
    "sizes = [real_total, fake_total]\n",
    "colors_pie = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%',\n",
    "                                     startangle=90, textprops={'fontweight': 'bold'})\n",
    "ax.set_title('Real vs Fake Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 3: Average Resolution\n",
    "ax = axes[1, 0]\n",
    "resolutions = []\n",
    "labels_res = []\n",
    "\n",
    "for stats, label in [(ff_real_stats, 'FF Real'), (ff_fake_stats, 'FF Fake'),\n",
    "                      (celebdf_real_stats, 'Celeb Real'), (celebdf_fake_stats, 'Celeb Fake')]:\n",
    "    if stats['count'] > 0:\n",
    "        resolutions.append(stats['avg_width'] * stats['avg_height'] / 1e6)  # Megapixels\n",
    "        labels_res.append(label)\n",
    "\n",
    "bars = ax.barh(labels_res, resolutions, color=colors[:len(resolutions)], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Resolution (Megapixels)', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Average Image Resolution', fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for bar, res in zip(bars, resolutions):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{res:.2f}MP', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 4: Quality Scores\n",
    "ax = axes[1, 1]\n",
    "quality_names = list(quality_metrics.keys())\n",
    "overall_scores = [quality_metrics[name]['overall_score'] for name in quality_names]\n",
    "\n",
    "bars = ax.barh(quality_names, overall_scores, color=colors[:len(quality_names)], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Quality Score', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Dataset Quality Assessment', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 1.0)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, overall_scores):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{score:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"   Saved: dataset_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION 2: Resolution Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä Figure 2: Resolution Analysis\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('Image Resolution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Collect resolution data\n",
    "all_widths = []\n",
    "all_heights = []\n",
    "all_labels = []\n",
    "\n",
    "for images, label in [(ff_real, 'FF Real'), (ff_fake, 'FF Fake'),\n",
    "                       (celebdf_real, 'Celeb Real'), (celebdf_fake, 'Celeb Fake')]:\n",
    "    if images:\n",
    "        widths = [img['width'] for img in images]\n",
    "        heights = [img['height'] for img in images]\n",
    "        all_widths.extend(widths)\n",
    "        all_heights.extend(heights)\n",
    "        all_labels.extend([label] * len(images))\n",
    "\n",
    "# Plot 1: Width Distribution\n",
    "ax = axes[0]\n",
    "df_res = pd.DataFrame({'Width': all_widths, 'Dataset': all_labels})\n",
    "for dataset in df_res['Dataset'].unique():\n",
    "    data = df_res[df_res['Dataset'] == dataset]['Width']\n",
    "    ax.hist(data, bins=20, alpha=0.5, label=dataset, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Image Width (pixels)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Width Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Height Distribution\n",
    "ax = axes[1]\n",
    "df_res = pd.DataFrame({'Height': all_heights, 'Dataset': all_labels})\n",
    "for dataset in df_res['Dataset'].unique():\n",
    "    data = df_res[df_res['Dataset'] == dataset]['Height']\n",
    "    ax.hist(data, bins=20, alpha=0.5, label=dataset, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Image Height (pixels)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Height Distribution', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'resolution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"   Saved: resolution_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION 3: File Size Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nüìä Figure 3: File Size Analysis\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Collect file size data\n",
    "size_data = []\n",
    "for images, label in [(ff_real, 'FF Real'), (ff_fake, 'FF Fake'),\n",
    "                       (celebdf_real, 'Celeb Real'), (celebdf_fake, 'Celeb Fake')]:\n",
    "    if images:\n",
    "        sizes = [img['size'] / 1024 for img in images]  # Convert to KB\n",
    "        size_data.append(sizes)\n",
    "    else:\n",
    "        size_data.append([0])\n",
    "\n",
    "# Create box plot\n",
    "bp = ax.boxplot(size_data, labels=['FF Real', 'FF Fake', 'Celeb Real', 'Celeb Fake'],\n",
    "                patch_artist=True, notch=True, showfliers=True)\n",
    "\n",
    "# Color the boxes\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_ylabel('File Size (KB)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('File Size Distribution by Dataset', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'file_size_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"   Saved: file_size_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# EXPORT STATISTICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. EXPORTING STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compile all statistics\n",
    "dataset_statistics = {\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'datasets': {\n",
    "        'faceforensics': {\n",
    "            'real': ff_real_stats,\n",
    "            'fake': ff_fake_stats\n",
    "        },\n",
    "        'celebdf': {\n",
    "            'real': celebdf_real_stats,\n",
    "            'fake': celebdf_fake_stats\n",
    "        },\n",
    "        'synthetic_attacks': synthetic_stats\n",
    "    },\n",
    "    'quality_metrics': quality_metrics,\n",
    "    'summary': {\n",
    "        'total_images': len(ff_real) + len(ff_fake) + len(celebdf_real) + len(celebdf_fake),\n",
    "        'total_real': len(ff_real) + len(celebdf_real),\n",
    "        'total_fake': len(ff_fake) + len(celebdf_fake),\n",
    "        'balance_ratio': (len(ff_fake) + len(celebdf_fake)) / (len(ff_real) + len(celebdf_real)) if (len(ff_real) + len(celebdf_real)) > 0 else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "stats_file = RESULTS_DIR / 'dataset_statistics.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(dataset_statistics, f, indent=2, default=str)\n",
    "\n",
    "print(f\" Statistics exported to: {stats_file}\")\n",
    "\n",
    "# Create markdown summary\n",
    "summary_md = f\"\"\"# Dataset Analysis Summary\n",
    "\n",
    "**Generated:** {dataset_statistics['timestamp']}\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Total Images:** {dataset_statistics['summary']['total_images']}\n",
    "- **Real Images:** {dataset_statistics['summary']['total_real']}\n",
    "- **Fake Images:** {dataset_statistics['summary']['total_fake']}\n",
    "- **Balance Ratio:** {dataset_statistics['summary']['balance_ratio']:.3f} (Fake/Real)\n",
    "\n",
    "## Dataset Breakdown\n",
    "\n",
    "### FaceForensics++\n",
    "- Real: {ff_real_stats['count']} images\n",
    "- Fake: {ff_fake_stats['count']} images\n",
    "- Avg Resolution (Real): {int(ff_real_stats['avg_width']) if ff_real_stats['count'] > 0 else 0}x{int(ff_real_stats['avg_height']) if ff_real_stats['count'] > 0 else 0}\n",
    "- Avg Resolution (Fake): {int(ff_fake_stats['avg_width']) if ff_fake_stats['count'] > 0 else 0}x{int(ff_fake_stats['avg_height']) if ff_fake_stats['count'] > 0 else 0}\n",
    "\n",
    "### Celeb-DF\n",
    "- Real: {celebdf_real_stats['count']} images\n",
    "- Fake: {celebdf_fake_stats['count']} images\n",
    "- Avg Resolution (Real): {int(celebdf_real_stats['avg_width']) if celebdf_real_stats['count'] > 0 else 0}x{int(celebdf_real_stats['avg_height']) if celebdf_real_stats['count'] > 0 else 0}\n",
    "- Avg Resolution (Fake): {int(celebdf_fake_stats['avg_width']) if celebdf_fake_stats['count'] > 0 else 0}x{int(celebdf_fake_stats['avg_height']) if celebdf_fake_stats['count'] > 0 else 0}\n",
    "\n",
    "### Synthetic Attacks\n",
    "- Scenarios: {synthetic_stats['scenarios']}\n",
    "- Avg Submissions/Attack: {synthetic_stats['avg_submissions_per_attack']:.1f}\n",
    "\n",
    "## Quality Assessment\n",
    "\n",
    "| Dataset | Overall Score |\n",
    "|---------|---------------|\n",
    "| FF Real | {quality_metrics.get('FF Real', {}).get('overall_score', 0):.3f} |\n",
    "| FF Fake | {quality_metrics.get('FF Fake', {}).get('overall_score', 0):.3f} |\n",
    "| Celeb Real | {quality_metrics.get('Celeb Real', {}).get('overall_score', 0):.3f} |\n",
    "| Celeb Fake | {quality_metrics.get('Celeb Fake', {}).get('overall_score', 0):.3f} |\n",
    "\n",
    "## Research Implications\n",
    "\n",
    "1. **Dataset Balance:** {\"Good\" if 0.8 <= dataset_statistics['summary']['balance_ratio'] <= 1.2 else \"Needs attention\"}\n",
    "2. **Sample Size:** {\"Sufficient\" if dataset_statistics['summary']['total_images'] >= 100 else \"Limited\"}\n",
    "3. **Quality:** {\"High\" if np.mean([m['overall_score'] for m in quality_metrics.values()]) > 0.7 else \"Acceptable\"}\n",
    "\n",
    "## Generated Figures\n",
    "\n",
    "1. `dataset_distribution.png` - Dataset overview and statistics\n",
    "2. `resolution_analysis.png` - Image resolution distributions\n",
    "3. `file_size_analysis.png` - File size comparisons\n",
    "\n",
    "---\n",
    "*Analysis generated by corruption-reporting-prototype evaluation framework*\n",
    "\"\"\"\n",
    "\n",
    "summary_file = RESULTS_DIR / 'dataset_analysis_summary.md'\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(f\" Summary exported to: {summary_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Dataset Analysis Summary\n",
    "========================\n",
    "\n",
    "üìä Datasets Analyzed:\n",
    "   ‚Ä¢ FaceForensics++: {len(ff_real) + len(ff_fake)} images ({len(ff_real)} real, {len(ff_fake)} fake)\n",
    "   ‚Ä¢ Celeb-DF: {len(celebdf_real) + len(celebdf_fake)} images ({len(celebdf_real)} real, {len(celebdf_fake)} fake)\n",
    "   ‚Ä¢ Synthetic Attacks: {synthetic_stats['scenarios']} scenarios\n",
    "   ‚Ä¢ Total: {dataset_statistics['summary']['total_images']} images\n",
    "\n",
    "üìà Quality Metrics:\n",
    "   ‚Ä¢ Average Quality Score: {np.mean([m['overall_score'] for m in quality_metrics.values()]):.3f}\n",
    "   ‚Ä¢ Dataset Balance Ratio: {dataset_statistics['summary']['balance_ratio']:.3f}\n",
    "\n",
    "üìÅ Outputs Generated:\n",
    "   ‚Ä¢ 3 visualization figures\n",
    "   ‚Ä¢ JSON statistics file\n",
    "   ‚Ä¢ Markdown summary report\n",
    "\n",
    " All analysis complete and saved!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
